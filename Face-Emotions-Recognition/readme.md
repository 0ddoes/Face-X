# Facial-Emotion-Recognition

Facial emotion recognition is the process of detecting human emotions from facial expressions. The human brain recognizes emotions automatically, and software has now been developed that can recognize emotions as well. This technology is becoming more accurate all the time, and will eventually be able to read emotions as well as our brains do. 

AI can detect emotions by learning what each facial expression means and applying that knowledge to the new information presented to it. Emotional artificial intelligence, or emotion AI, is a technology that is capable of reading, imitating, interpreting, and responding to human facial expressions and emotions. 

## About
## Eye üëÅÔ∏è  Blinker-Counter
![image](https://user-images.githubusercontent.com/78999467/110669344-57cd2e80-81f4-11eb-9637-c8f5c3a267bf.png)


##### In terms of blink detection, we are only interested in two sets of facial structures ‚Äî the eyes.
The first step in building a blink detector is to perform facial landmark detection to localize the eyes in a given frame from a video stream.
Each eye is represented by coordinates, starting at the left corner of the eye (as if you were looking at the person), and then working clockwise around the remainder of the region.
Once we have the eye aspect ratio, we can threshold it to determine if a person is blinking ‚Äî the eye aspect ratio will remain approximately constant when the eyes are open and then will rapidly approach zero during a blink, then increase again as the eye opens.

## Facial-Expression-Recognition-using-custom-CNN üòÑ :angry: üò¢ üò≠ üòÜ 
![image](https://user-images.githubusercontent.com/78999467/110669184-2f453480-81f4-11eb-9ac2-611dd5754f92.png)
![image](https://user-images.githubusercontent.com/78999467/110669183-2f453480-81f4-11eb-9a3a-a971bb7a9e95.png)

A convolutional neural network (CNN) is the most popular way of analyzing images. The proposed method is based on a two-level CNN framework. The first level recommended is background removal [29], used to extract emotions from an image, as shown in Fig. 1. Here, the conventional CNN network module is used to extract the primary expressional vector (EV). The expressional vector (EV) is generated by tracking down relevant facial points of importance. EV is directly related to changes in expression. The EV is obtained using a basic perceptron unit applied to a background-removed face image. In the proposed FERC model, we also have a non-convolutional perceptron layer as the last stage. Each of the convolutional layers receives the input data (or image), transforms it, and then outputs it to the next level. This transformation is a convolution operation. All the convolutional layers used are capable of pattern detection. Within each convolutional layer, four filters were used. The input image fed to the first-part CNN (used for background removal) generally consists of shapes, edges, textures, and objects along with the face. The **edge detector, circle detector, and corner detector filters** are used at the start of the convolutional layer 1. Once the face has been detected, the second-part CNN filter catches facial features, such as eyes, ears, lips, nose, and cheeks. The edge detection filters used in this layer. The second-part CNN consists of layers with 3√ó3 kernel matrix, e.g., [0.25, 0.17, 0.9; 0.89, 0.36, 0.63; 0.7, 0.24, 0.82]. These numbers are selected between 0 and 1 initially. These numbers are optimized for EV detection, based on the ground truth we had, in the supervisory training dataset. Once the filter is tuned by supervisory learning, it is then applied to the background-removed face (i.e., on the output image of the first-part CNN), for detection of different facial parts (e.g., eye, lips. nose, ears, etc.)

## Smile üòÑ  Percentage Detection
![image](https://user-images.githubusercontent.com/78999467/110666784-bc3abe80-81f1-11eb-95c6-698f8dd2116d.png)
![image](https://user-images.githubusercontent.com/78999467/110666785-bc3abe80-81f1-11eb-81a2-e8d1b86c7ecf.png)

*We propose a method to automatically refine for real-time usage.*
The smile detect algorithm is as follows:
1. Detect the first human face in the first image frame
and locate the twenty standard facial features
position.
2. In every image frame, use optical flow to track the
position of left mouth corner and right mouth
corner with an accuracy of 0.01 pixels and update the
standard facial feature position by face tracking
and detection.
3. If x-direction distance between the tracked left
mouth corner and right mouth corner is larger than
the standard distance plus a threshold Tsmile, then
we claim a smile detected.
4. Repeat from Step 2 to Step 3
## face-emotions-recognition-using-deep-learning
![image](https://user-images.githubusercontent.com/78999467/110668788-c8278000-81f3-11eb-81ec-e12d728b1ead.png)

An emotion recognition system can be built by utilizing the benefits of deep learning and different applications such as feedback analysis, face unlocking, etc. can be implemented with good accuracy. The main focus of this work is to create a Deep Convolutional Neural Network (DCNN) model that classifies 5 different human facial emotions. The model is trained, tested, and validated using the manually collected image dataset. We aim to construct a system that captures real-world facial images through the front camera on a laptop. The system is capable of processing/recognizing the captured image and predict a result in real-time. In this system, we exploit the power of the **deep learning technique** to learn a facial emotion recognition (FER) model based on a set of labeled facial images. Finally, experiments are conducted to evaluate our model using a largely used public database. A 3D facial emotion recognition model using a deep learning technique is proposed. In the deep learning architecture, two convolution layers and a pooling layer are used. Pooling is performed after convolution operation. The sigmoid activation function is used to obtain the probabilities for different classes of human faces. In order to validate the performance of the deep learning-based face recognition model, the Kaggle dataset is used. The accuracy of the model is approximately 65% which is less than the other techniques used for facial emotion recognition. Despite dramatic improvements in representation precision attributable to the non-linearity of profound image representations. 
üí´ üí´ üí´ 
